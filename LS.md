<h3>Title: Large Language Models and their comparison</h3>

<p align="justify"><b>Aim:</b> The aim of this literature survey is to provide a comprehensive review of large language models in natural language processing. It will focus on analyzing the architectures, training techniques, and applications of prominent models such as GPT and BERT. The survey aims to compare the performance, limitations, and impact of these models, while identifying emerging trends and open challenges in the field. </p>

--------
<h3>Introduction:</h3>
<p align="justify"> LLMs are based on neural networks, which are a type of machine learning algorithm that can process data quickly and accurately. They are trained on large datasets of text, such as books, articles, and websites, and use this data to learn patterns and relationships between words and phrases. This allows them to generate natural-sounding sentences and paragraphs that are similar to those written by humans. Unlike other AI technologies, LLMs do not require pre-programmed rules or labeled data, which makes them more flexible and adaptable to different tasks. </p>
<p align="justify">LLMs work by using a type of neural network called a transformer, which is designed to process sequential data such as text. The transformer consists of multiple layers of processing units that can learn patterns and relationships between words and phrases in a text dataset. During training, the transformer is fed large amounts of text data and learns to predict the next word in a sentence based on the words that came before it. This process allows the transformer to learn the underlying structure of language and generate natural-sounding text. Once the LLM is trained, it can be used to generate text in response to a given prompt or question, or to perform other natural language processing tasks such as sentiment analysis or language translation.</p>

--------
<h3>Model Architecture:</h3>
